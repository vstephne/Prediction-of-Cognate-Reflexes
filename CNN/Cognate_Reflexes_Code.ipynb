{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**COGNATE REFLEXES PREDICTION**"
      ],
      "metadata": {
        "id": "0EuDH_2FxjMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install tensorflow version 2.5\n",
        "#we faced an issue when used other tensorflow versions\n",
        "!pip install tensorflow=='2.5'\n",
        "#similarly pandas and keras import on .ipnyb(Google Colab) were required\n",
        "!pip install pandas\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "6mi5k-Btqjq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing appropriate modules\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Sequence\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "N90cMy8EYw6N"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the cognate sets and the vocab from the training file.\n",
        "def buildDataset(data_dir,train_path):\n",
        "  #create vocabulary\n",
        "  #initialize empty vocabulary\n",
        "  vocabulary = set()\n",
        "  cogsets = []\n",
        "  #training file path\n",
        "  filepath = os.path.join(data_dir, train_path)\n",
        "  print('Get Training data :', filepath)\n",
        "  #open training file\n",
        "  with open(filepath, 'r', encoding='utf-8') as fp:\n",
        "    next(fp)\n",
        "    for line in fp:\n",
        "      parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "      for p in parts:\n",
        "        for c in p.split():\n",
        "          vocabulary.add(c)\n",
        "      cogsets.append([p.strip() for p in parts])\n",
        "  #adding possible extra words\n",
        "  vocabulary = ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>'] + sorted(\n",
        "      list(vocabulary))\n",
        "  print(\"COGSETS:\",cogsets)\n",
        "  print(\"VOCAB:\",vocabulary)\n",
        "  #returning cognets and vocabulary\n",
        "  return cogsets, vocabulary"
      ],
      "metadata": {
        "id": "kLWlTZ5gY9nu"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to read from hyper parameters file\n",
        "def hyperparameters(checkpoint_dir):\n",
        "  hyperparam_path = os.path.join(checkpoint_dir, 'hparams.json')\n",
        "  with open(hyperparam_path, 'r') as fp:\n",
        "    hyperparams = json.load(fp)\n",
        "    print(\"HPARAMS:\",hyperparams,\"\\n\")\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "nMfM0EGRZUzP"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_dataset(cognatesets):\n",
        "  #each of our datasert has multiple languages we are copying vocab for each dataset \n",
        "  #sampleList would be a list of list of vocabs for each language\n",
        "  number_languages = len(cognatesets[0])\n",
        "  sampleList = []\n",
        "  for val in cognatesets:\n",
        "    sample = []\n",
        "    for i in range(number_languages):\n",
        "      if val[i]:\n",
        "        sample.append(val[i])\n",
        "      else:\n",
        "        sample.append('<BLANK>')\n",
        "    sampleList.append(sample)\n",
        "  random.shuffle(sampleList)\n",
        "  return sampleList"
      ],
      "metadata": {
        "id": "zk7nibQeaDqc"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are using function to take trainnig data mask few words convert it into tensor and return\n",
        "def create_dataset_forTrain(samples, blen, number_languages, maximumlen, charecterToIndex):\n",
        "  def language_generator():\n",
        "    while True:\n",
        "      for ic in samples:\n",
        "        inp = []\n",
        "        result = []\n",
        "        inpHiddenVal = []\n",
        "        targetMask = []\n",
        "        correct = [i for i in range(len(ic)) if ic[i] != '<BLANK>']\n",
        "        number = random.randint(1, len(correct))\n",
        "        exist = random.sample(correct, number)\n",
        "        for i in range(len(ic)):\n",
        "          template = [charecterToIndex['<BLANK>']] * maximumlen\n",
        "          seq = [charecterToIndex['<BOS>']] + [\n",
        "              charecterToIndex[c] if c in charecterToIndex else charecterToIndex['<UNK>']\n",
        "              for c in ic[i].split()\n",
        "          ] + [charecterToIndex['<EOS>']]\n",
        "          for j in range(min(len(seq), maximumlen)):\n",
        "            template[j] = seq[j]\n",
        "          result.append(template)\n",
        "          inp.append(template)\n",
        "          if i in correct:\n",
        "            targetMask.append([1.0] * maximumlen)\n",
        "          else:\n",
        "            targetMask.append([0.0] * maximumlen)\n",
        "          if i in exist:\n",
        "            inpHiddenVal.append([1.0] * maximumlen)\n",
        "          else:\n",
        "            inpHiddenVal.append([0.0] * maximumlen)\n",
        "        inp = tf.constant([inp], dtype='int32')\n",
        "        result = tf.constant([result], dtype='int32')\n",
        "        inpHiddenVal = tf.constant([inpHiddenVal], dtype='float32')\n",
        "        target_mask = tf.constant([targetMask], dtype='float32')\n",
        "\n",
        "        yield (inp, result, inpHiddenVal, target_mask)\n",
        "\n",
        "  return tf.data.Dataset.from_generator(\n",
        "      language_generator,\n",
        "      output_signature=(tf.TensorSpec(\n",
        "          shape=(blen, number_languages, maximumlen), dtype='int32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(blen, number_languages, maximumlen),\n",
        "                            dtype='int32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(blen, number_languages, maximumlen),\n",
        "                            dtype='float32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(blen, number_languages, maximumlen),\n",
        "                            dtype='float32')))"
      ],
      "metadata": {
        "id": "O-YTgUDhYxcS"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating infiller model\n",
        "class Model_Cognet(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocabularySize, hyperparams, bsize, number_languages, mLen):\n",
        "    super(Model_Cognet, self).__init__()\n",
        "    self.batch = bsize\n",
        "    self.units = hyperparams['filters']\n",
        "    self.kenelSize = hyperparams['kernel_width']\n",
        "    self.vocab_size = vocabularySize\n",
        "    self.embedDim = hyperparams['embedding_dim']\n",
        "    self.nlangs = number_languages\n",
        "    self.maximumSize = mLen\n",
        "    self.posVal = hyperparams['sfactor']\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,self.embedDim)    \n",
        "    self.convolution = tf.keras.layers.Conv2D(filters=self.units, kernel_size=(number_languages,self.kenelSize))\n",
        " \n",
        "    if hyperparams['nonlinearity'] == 'leaky_relu':\n",
        "      self.activation = tf.keras.layers.LeakyReLU()\n",
        "    elif hyperparams['nonlinearity'] == 'relu':\n",
        "      self.activation = tf.keras.layers.ReLU()\n",
        "    else:\n",
        "      self.activation = tf.keras.layers.Activation('tanh')\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(hyperparams['dropout'])\n",
        "\n",
        "    self.deconvolution = tf.keras.layers.Conv2DTranspose(filters=self.vocab_size, kernel_size=(number_languages, self.kenelSize))\n",
        "\n",
        "  def call(self, inp, inpMask, training):\n",
        "    \n",
        "    rmask = tf.repeat(inpMask, self.embedDim, axis=-1)\n",
        "    rmask = tf.reshape(\n",
        "        rmask,\n",
        "        shape=(self.batch, self.nlangs, self.maximumSize,\n",
        "               self.embedDim))\n",
        "\n",
        "    \n",
        "    inp = self.embedding(inp)\n",
        "    inp = inp * rmask\n",
        "\n",
        "    \n",
        "    sfactor = (self.nlangs * self.maximumSize) / tf.math.reduce_sum(inpMask)\n",
        "    if self.posVal == 'inputs':\n",
        "      inp = inp * sfactor\n",
        "\n",
        "    #convolution layer\n",
        "    inp = self.convolution(inp)\n",
        "    if self.posVal == 'conv':\n",
        "      inp = inp * sfactor\n",
        "    inp = self.dropout(inp, training=training)\n",
        "    inp = self.activation(inp)\n",
        "\n",
        "    #deconvolution layer\n",
        "    netOut = self.deconvolution(inp)\n",
        "\n",
        "    return netOut"
      ],
      "metadata": {
        "id": "DY-LLJt6a2Zi"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "#calculation of loss\n",
        "def evaluationLoss(input, prediction, mask):\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')\n",
        "  losses = cross_entropy(y_true=input, y_pred=prediction)\n",
        "  losses = mask * losses\n",
        "  losses = tf.reduce_sum(losses)\n",
        "  return losses"
      ],
      "metadata": {
        "id": "xeKTPxOJbRB_"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def trainperstep(Model_Cognet, optimizer, input, input_mask, target, target_mask):\n",
        "  \"\"\"Single training step.\"\"\"\n",
        "  lossperstep = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = Model_Cognet(input, input_mask, training=True)\n",
        "    lossperstep = evaluationLoss(target, logits, target_mask)\n",
        "  variables = Model_Cognet.trainable_variables\n",
        "  gradients = tape.gradient(lossperstep, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return lossperstep"
      ],
      "metadata": {
        "id": "Vvo-9v-ibPhu"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function helps while calculating errors it gives where target starts and ends\n",
        "def evaluate_cognate_set(Model_Cognet, cognateset, chr2idx, max_length):\n",
        "  \"\"\"Evaluates given cognate set.\"\"\"\n",
        "  target_index = 0\n",
        "  inputs = []\n",
        "  input_mask = []\n",
        "  # Find possible target positions\n",
        "  for i, p in enumerate(cognateset):\n",
        "    if p.strip():\n",
        "      if p == '<TARGET>':\n",
        "        target_index = i\n",
        "        inputs.append([chr2idx['<TARGET>']] * max_length)\n",
        "        input_mask.append([0.0] * max_length)\n",
        "      else:\n",
        "        seq = [chr2idx['<BOS>']] + [\n",
        "            chr2idx[c] if c in chr2idx else chr2idx['<UNK>']\n",
        "            for c in p.split()\n",
        "        ] + [chr2idx['<EOS>']]\n",
        "        template = [chr2idx['<BLANK>']] * max_length\n",
        "        for j in range(min(len(seq), max_length)):\n",
        "          template[j] = seq[j]\n",
        "        inputs.append(template)\n",
        "        input_mask.append([1.0] * max_length)\n",
        "    else:\n",
        "      inputs.append([chr2idx['<BLANK>']] * max_length)\n",
        "      input_mask.append([0.0] * max_length)\n",
        "\n",
        "  inputs = tf.constant([inputs], dtype='int32')\n",
        "  input_mask = tf.constant([input_mask], dtype='float32')\n",
        "\n",
        "  logits = Model_Cognet(inputs, input_mask, training=False)\n",
        "  trow = tf.math.argmax(logits[0, target_index, :, :], axis=-1)\n",
        "  return trow.numpy()"
      ],
      "metadata": {
        "id": "UiECAcY_bcci"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(modelCognate, cognateset, charToIndex, maxLen, indexToChar):\n",
        "  eval = evaluate_cognate_set(modelCognate, cognateset, charToIndex, maxLen)\n",
        "  eval = list(eval)\n",
        "  eval = ' '.join([\n",
        "      indexToChar[x] for x in eval if indexToChar[x] not in\n",
        "      ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>']\n",
        "  ])\n",
        "  return eval"
      ],
      "metadata": {
        "id": "bc-rQcjqbbir"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trains the model\n",
        "def train_model(data_dir,checkpoint_dir,train_path,dev_path,dev_sol_path):\n",
        "  cognatesets, vocab = buildDataset(data_dir,train_path)\n",
        "  chr2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "  idx2chr = {i: vocab[i] for i in range(len(vocab))}\n",
        "  languages = len(cognatesets[0])\n",
        "  hparams = hyperparameters(checkpoint_dir)\n",
        "  vocab_size = len(vocab)\n",
        "  all_samples = extend_dataset(cognatesets)\n",
        "\n",
        "  # Getting dev data from file\n",
        "  dev = []\n",
        "  filepath = os.path.join(data_dir, dev_path)\n",
        "  with open(filepath, 'r', encoding='utf-8') as devfile:\n",
        "    #header has languages so skip\n",
        "    next(devfile)\n",
        "    for line in devfile:\n",
        "      parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "      parts = ['<TARGET>' if p == '?' else p for p in parts]\n",
        "      dev.append(parts)\n",
        "\n",
        "  # Get dev solution data\n",
        "  dev_sol = []\n",
        "  filepath = os.path.join(data_dir, dev_sol_path)\n",
        "  with open(filepath, 'r', encoding='utf-8') as devsolfile:\n",
        "    # Header has languages so skip\n",
        "    next(devsolfile)\n",
        "    for line in devsolfile:\n",
        "      parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "      dev_sol.append(''.join(parts).strip())\n",
        "  steps_per_epoch = 500\n",
        "  batch_size = 1\n",
        "  max_length = 20\n",
        "  #Initially no vocabs are createn so false\n",
        "  vocab_written = False\n",
        "\n",
        "  #calling model\n",
        "  infiller = Model_Cognet(vocab_size, hparams, batch_size, languages, max_length)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  if checkpoint_dir:\n",
        "      checkpoint = tf.train.Checkpoint(optimizer=optimizer, infiller=infiller)\n",
        "\n",
        "  logging.info('Training the model ...')\n",
        "  train_dataset = create_dataset_forTrain(all_samples, batch_size, languages,\n",
        "                                      max_length, chr2idx)\n",
        "  err = None\n",
        "\n",
        "  for epoch in range(500):\n",
        "    start = time.time()\n",
        "\n",
        "    tloss = 0\n",
        "\n",
        "    for (_, (input, target, input_mask,\n",
        "             targ_mask)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "      loss_per_batch = trainperstep(infiller, optimizer, input, input_mask, target,\n",
        "                              targ_mask)\n",
        "      tloss += loss_per_batch\n",
        "      # print(\"lossssssss\",total_loss)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        tloss / steps_per_epoch))\n",
        "\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    # Evaluate on dev set:\n",
        "    devSetErrors = [0 for l in range(languages)]\n",
        "    devSetTotal = [0 for l in range(languages)]\n",
        "    totalErr = 0\n",
        "    for dset, dsol in zip(dev, dev_sol):\n",
        "      trgt_idx = dset.index('<TARGET>')\n",
        "      devSetTotal[trgt_idx] += 1\n",
        "      pred = translate(infiller, dset, chr2idx, max_length, idx2chr)\n",
        "      if pred != dsol:\n",
        "        devSetErrors[trgt_idx] += 1\n",
        "        totalErr += 1\n",
        "    devSetErrors = [x / y for x, y in zip(devSetErrors, devSetTotal) if y != 0]\n",
        "    mean_accuracy = np.mean(devSetErrors)\n",
        "\n",
        "    if not err or mean_accuracy <= err:\n",
        "      print('ERROR_UPDATE:', devSetErrors)\n",
        "      if checkpoint_dir:\n",
        "        checkpoint.save('/content/checkpoint_dir/best_model.ckpt')\n",
        "        if not vocab_written:\n",
        "          hparams = hyperparameters(checkpoint_dir)\n",
        "          hparams['embedding_dim'] = hparams[\"embedding_dim\"]\n",
        "          hparams['kernel_width'] = hparams[\"kernel_width\"]\n",
        "          hparams['filters'] = hparams[\"filters\"]\n",
        "          hparams['dropout'] = hparams[\"dropout\"]\n",
        "          hparams['nonlinearity'] = hparams[\"nonlinearity\"]\n",
        "          hparams['sfactor'] = hparams[\"sfactor\"]\n",
        "          with open(checkpoint_dir + 'hparams.json', 'w') as vfile:\n",
        "            json.dump(hparams, vfile)\n",
        "          with open(\n",
        "              checkpoint_dir + '/vocab.txt', 'w', encoding='utf-8') as vfile:\n",
        "            for v in vocab:\n",
        "              vfile.write(v + '\\n')\n",
        "          vocab_written = True\n",
        "      best_error = mean_accuracy\n",
        "    print(best_error, mean_accuracy, '\\n')\n",
        "\n",
        "  logging.info('Done. Shutting down ...')"
      ],
      "metadata": {
        "id": "AEMaXzE5mBB_"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model('/content/data_dir','/content/checkpoint_dir','training-mod-0.10_01.tsv','dev-0.10_01.tsv','dev_solutions-0.10_01.tsv')"
      ],
      "metadata": {
        "id": "LuiaBYNzcPyy"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#readin data from aldready created vocab file\n",
        "def vocablist(checkpoint_dir):\n",
        "  file_path = os.path.join(checkpoint_dir, 'vocab.txt')\n",
        "  if not file_path:\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  logging.info('Loading vocab from %s ...', file_path)\n",
        "  with open(file_path, 'r', encoding='utf8') as f:\n",
        "    vocab = [symbol.strip() for symbol in f if symbol]\n",
        "  logging.info('%d symbols loaded.', len(vocab))\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "4ilsGIsTnaDg"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing data\n",
        "def test(data_dir,checkpoint_dir):\n",
        "  hparams = hyperparameters(checkpoint_dir)\n",
        "  vocab = vocablist(checkpoint_dir)\n",
        "  chr2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "  idx2chr = {i: vocab[i] for i in range(len(vocab))}\n",
        "  vocab_size = len(vocab)\n",
        "  batch_size = 1\n",
        "  max_length = 20\n",
        "\n",
        "  test_filepath = os.path.join(data_dir, 'test-0.10.tsv')\n",
        "  preds_filepath = os.path.join(data_dir, 'pred-0.10.tsv')\n",
        "\n",
        "  with open(test_filepath, 'r', encoding='utf-8') as tfile:\n",
        "    languages = len(next(tfile).strip('\\n').split('\\t')) - 1\n",
        "\n",
        "\n",
        "  best_ckpt_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  if not best_ckpt_path:\n",
        "    raise ValueError('No checkpoint available')\n",
        "  logging.info('Restoring from checkpoint %s ...', best_ckpt_path)\n",
        "  infiller = Model_Cognet(vocab_size, hparams, batch_size, languages, max_length)\n",
        "  checkpoint = tf.train.Checkpoint(infiller=infiller)\n",
        "  checkpoint.restore(best_ckpt_path).expect_partial()\n",
        "\n",
        "  logging.info('Generating predictions and saving results...')\n",
        "  with open(preds_filepath, 'w', encoding='utf-8') as predfile:\n",
        "    with open(test_filepath, 'r', encoding='utf-8') as testfile:\n",
        "      # Copy the header.\n",
        "      predfile.write(next(testfile))\n",
        "      for line in testfile:\n",
        "        parts = line.strip('\\n').split('\\t')\n",
        "        testset = ['<TARGET>' if p == '?' else p for p in parts[1:]]\n",
        "        target_index = testset.index('<TARGET>')\n",
        "        pred = translate(infiller, testset, chr2idx, max_length, idx2chr)\n",
        "        row = ['' for p in parts]\n",
        "        row[0] = parts[0]\n",
        "        row[target_index + 1] = pred\n",
        "        predfile.write('\\t'.join(row) + '\\n')"
      ],
      "metadata": {
        "id": "093fZ15zcRXM"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test('/content/data_dir','/content/checkpoint_dir')"
      ],
      "metadata": {
        "id": "0kFoAIwwrnNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import is required for evaluation\n",
        "!pip install lingrex\n",
        "!pip install lingpy"
      ],
      "metadata": {
        "id": "s8j_UY07tvyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lingrex imports\n",
        "from lingrex.util import bleu_score\n",
        "from lingpy import *\n",
        "from lingpy.evaluate.acd import _get_bcubed_score as bcubed_score\n",
        "from tabulate import tabulate\n",
        "from collections import defaultdict\n",
        "from lingpy.sequence.ngrams import get_n_ngrams\n",
        "import math"
      ],
      "metadata": {
        "id": "pFQTxwu8_83x"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loads each column of solution and pred file separately\n",
        "def load_files(path):\n",
        "    \"\"\"\n",
        "    Helper function for simplified cognate formats.\n",
        "    \"\"\"\n",
        "    data = csv2list(path, strip_lines=False)\n",
        "    header = data[0]\n",
        "    languages = header[1:]\n",
        "    out = {}\n",
        "    sounds = defaultdict(lambda : defaultdict(list))\n",
        "    for row in data[1:]:\n",
        "        out[row[0]] = {}\n",
        "        for language, entry in zip(languages, row[1:]):\n",
        "            out[row[0]][language] = entry.split()\n",
        "            for i, sound in enumerate(entry.split()):\n",
        "                sounds[sound][language] += [[row[0], i]]\n",
        "    return languages,sounds, out"
      ],
      "metadata": {
        "id": "2dTmkLgPtzmL"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating various metrics\n",
        "def compare_words(firstfile, secondfile, report=True):\n",
        "    \"\"\"\n",
        "    Evaluate the predicted and attested words in two datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    (languages, soundsA, first), (languagesB, soundsB, last) = load_files(firstfile), load_cognate_file(secondfile)\n",
        "    print(\"///\",languages, soundsA, first)\n",
        "    all_scores = []\n",
        "    for language in languages:\n",
        "        scores = []\n",
        "        almsA, almsB = [], []\n",
        "        for key in first:\n",
        "            if language in first[key]:\n",
        "                entryA = first[key][language]\n",
        "                if \" \".join(entryA):\n",
        "                    try:\n",
        "                        # print(\"&&&&\",entryA)\n",
        "                        entryB = last[key][language]\n",
        "                        # print(\"####\",entryB)\n",
        "                    except KeyError:\n",
        "                        print(\"Missing entry {0} / {1} / {2}\".format(\n",
        "                            key, language, secondfile))\n",
        "                        entryB = \"\"\n",
        "                    if not entryB:\n",
        "                        entryB = (2 * len(entryA)) * [\"Ø\"]\n",
        "                    # print(entryA)\n",
        "                    # print(entryB)\n",
        "                    almA, almB, _ = nw_align(entryA, entryB)\n",
        "                    almsA += almA\n",
        "                    almsB += almB\n",
        "                    score = 0\n",
        "                    for a, b in zip(almA, almB):\n",
        "                        if a == b and a not in \"Ø?-\":\n",
        "                            pass\n",
        "                        elif a != b:\n",
        "                            score += 1\n",
        "                    scoreD = score / len(almA)\n",
        "                    bleu = bleu_score(entryA, entryB, n=4, trim=False)\n",
        "                    scores += [[key, entryA, entryB, score, scoreD, bleu]]\n",
        "        if scores:\n",
        "            p, r = bcubed_score(almsA, almsB), bcubed_score(almsB, almsA)\n",
        "            fs = 2 * (p*r) / (p+r)\n",
        "            all_scores += [[\n",
        "                language,\n",
        "                sum([row[-3] for row in scores])/len(scores),\n",
        "                sum([row[-2] for row in scores])/len(scores),\n",
        "                fs,\n",
        "                sum([row[-1] for row in scores])/len(scores)]]\n",
        "    all_scores += [[\n",
        "        \"TOTAL\", \n",
        "        sum([row[-4] for row in all_scores])/len(languages),\n",
        "        sum([row[-3] for row in all_scores])/len(languages),\n",
        "        sum([row[-2] for row in all_scores])/len(languages),\n",
        "        sum([row[-1] for row in all_scores])/len(languages),\n",
        "        ]]\n",
        "    if report:\n",
        "        print(\n",
        "                tabulate(\n",
        "                    all_scores, \n",
        "                    headers=[\n",
        "                        \"Language\", \"ED\", \"ED (Normalized)\", \n",
        "                        \"B-Cubed FS\", \"BLEU\"], floatfmt=\".3f\"))\n",
        "    return all_scores"
      ],
      "metadata": {
        "id": "9h8nLazmAKig"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_words('/content/data_dir/pred-0.10.tsv','/content/data_dir/solutions-0.10.tsv')"
      ],
      "metadata": {
        "id": "40B-pZRQbqVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}